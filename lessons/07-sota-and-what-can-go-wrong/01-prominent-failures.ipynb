{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias, Interpretability, and Risk in ML\n",
    "\n",
    "In part due to its close connection with mathematics, AI systems have a reputation of being more objective and less biased than humans. In this section I want to caution against this view. Many AI systems have been shown to both adopt and amplify existing societal biases.\n",
    "\n",
    "Bias can be injected into a deep learning system at multiple points throughout the development lifecycle. One of the biggest risks for any ML system is that the data itself encodes social biases. In this section we're going to discuss examples of how this has happened, and think about strategies for avoiding problems associated with the data collection process.\n",
    "\n",
    "AI systems come with their own risks, often exacerbated by the use of models that are difficult or impossible to intepret. AI and ML systems are increasingly being used to make decisions that impact people's lives, and as a result we must carefully consider the risks such software presents. \n",
    "\n",
    "In this section we'll examine three examples that eluicidate some of these risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon's Resume Scanner\n",
    "\n",
    "In 2018, Amazon [shut down a machine learning system it had built to scan resumes](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G). Because the system was discriminating against women.\n",
    "\n",
    "**So, lets discuss: how could this happen?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticial Considerations:\n",
    "\n",
    "**Class imbalance**: Amazon's technical workforce is skewed heavily towards men. A machine learning system trained on a random sample of Amazon engineers will get significantly more examples of men who got hired than women who got hired. The implicaction is a probability distribution that favors men. Think about it this way: if 90% of the \"good hire\" examples are men, then an ML model will very reasonabily learn to bet on men. \n",
    "\n",
    "**Recreating the past**: There are a host of reasons women are underrepresented in the technology workforce. The past is biased, and our machine learning model is trained on historical data. If the training and test datasets both reflect a baised history, the model's predictions will reflect that history. Remember, our ML models are basically answering the question, \"Given the historical data we fed into you, what outcome do you expect on this new data?\"  \n",
    "\n",
    "**Biased Evaluators**: The people evaluating the system were Amazon's mostly male engineering teams. Not that men cannot be impartial, but if the system demonstrates bias that the evaluators are used to, they might not recognize it as bias. Said another way, if 90% of your coworkers are men it might not surprise you when 90% of the predicted \"good hires\" are men. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### What to do about it?\n",
    " \n",
    " **Class imbalance**:\n",
    " \n",
    " * Collect more data! If you can collect more examples of the underrepresented class, you can balance out your dataset and make it reflect the outcomes you *want* not just the outcomes that already exist in the domain you're studying. \n",
    " * Change the performance metric. Focusing on alternate metrics like false positives, false negatives, precision, recall, F1 Score and Kappa can tease help a system perform better even with data that has imbalance. [Read about these metrics here](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/)\n",
    " * Resample from the dataset to balance the classes. If you have enough samples, you can balance your classes by throwing out some of the overrepresented data points (called undersampling). If you're low on samples you can make duplicates of the underrepresented class (called oversampling). It may seem simplistic, but these tactics have both been shown to improve performance in some cases.  \n",
    " * Generate synthetic data to balance things out. SMOTE is a popular tactic for oversampling through the use of synthetic data, for example by using Synthetic Minority Oversampling Technique [SMOTE paper](https://www.jair.org/index.php/jair/article/view/10302). [Example of using SMOTE in Python](https://beckernick.github.io/oversampling-modeling/)\n",
    "\n",
    "**Recreating the Past**: When you're making a machine learning system make sure to ask yourself if the data you have represents the outcomes you want. In descriptive statistics we're seeking to *describe what the data says* but when we apply that mindset to predictive modeling, esp. if that model will be used to guide a decision making process, we have to ask ourselves: **If our model just recreates the status quo, is that good enough?** if not, you may need to think about using another tactic. \n",
    "\n",
    "**Biased Evaluators**: Make sure lots of people with different backgrounds evaluate the results of a model. This is what academic peer review has always sought to do, and while that system is not perfect, there is plenty of evidence that having challenging and genuine disagreements over any finding or result generally improves the quality of any resulting paper. For examples of this research read [Wikipedia and the Wisdom of Polarized Crowds](http://nautil.us/issue/70/variables/wikipedia-and-the-wisdom-of-polarized-crowds) If possible, have one or more 3rd party look at the results! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wolf or Husky?\n",
    "\n",
    "A famous finding in machine learning literature was uncovered when researchers built a neural network that attempted to distinguish between wolves and huskies. Researchers demonstrated that the pixels that were most important to the model's predictions weren't the animals, but rather the background! \n",
    "\n",
    "![wolf or dog?](img/dog_wolf.png)\n",
    "\n",
    "**Lets discuss: What is the lesson of this finding?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical Considerations:\n",
    "\n",
    "* Machine learning models often excel at learning in ways that humans do not expect. This is one of their core strengths, but it is also a weakness especially when the human evaluators assume the model is learning from the same patterns that they would. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What To Do About It:\n",
    "\n",
    "* It is not safe to assume the model is doing what you think it is doing. Whenever possible, design tests, experiments, or techniques to figure explain your model's behavior. This can be a challenge, but there are existing frameworks, see [\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938) for an example. \n",
    "* Interpreting the predictions of neural networks is an enormous challenge and the subject of ongoing research. Do your best to think about all the features your models *MIGHT* be learning from when designing the model and experiments. You can train the same model on slightly different versions of the input data to see how some features of the data might be impacting the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Tay: The AI That Learned To Love Hitler\n",
    "\n",
    "In 2016, Microsoft deployed a machine learning system that was meant to mimic the way teenagers behave on Twitter. The AI was designed to continue learning as it had interactions with real people on Twitter. The whole premise is a little weird, but what happened when they deployed the system was even weirder. In less than 24 hours went from innocent tweets like, \"humans are super cool\" to tweeting: \"Repeat after me, Hitler did nothing wrong.\"\n",
    "\n",
    "**Lets Discuss: What might have caused Tay to learn these behaviors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical Considerations: \n",
    "\n",
    "* Twitter is not exactly a controlled environment. Nor is it known for its compassion and egalitarianism. Microsoft was opening a can of worms by essentially trusting Twitter users to act in good faith.\n",
    "* Machine learning systems are suseptible to intentional abuse! The tactic of using \"adversarial examples\" is well studied as [this OpenAI blog post describes](https://openai.com/blog/adversarial-example-research/). \n",
    "* Dynamic, uncontrolled, environments pose a major challenge to machine learning systems. As discussed above, they are built using data from the past, expecting them to perfom well in the future encodes the assumption that the past is a decent representation of the future. In dynamic systems where things are always changing, this isn't always a good assumption.  \n",
    "* This is **criticial** in things like spam and fraud detection. Fraudsters are constantly changing tactics to beat  the systems that detect fraud. The world is dynamic, and it's not safe to assume a system that works today will still work next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What To Do About It?\n",
    "\n",
    "* Don't let strangers from the internet prepare the dataset that your model trains on unless you're absolutely sure that you've already prepared for the worst.\n",
    "* Apply a hacker mentality to your machine learning models! Ask yourself, \"how could I break this?\" and then test the ideas you have and see if they do in fact break your model. Invite outsiders to play \"war games\" against your AI and incorporate the results into an expanding dataset.\n",
    "* If you're building a model that will interact in a dynamic environment make sure you regularly re-evaluate the model, and have other mechanisms in place for detecting that the model might be failing due to changes in the environment. \n",
    "* Always consider resiliaincy to change when building models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
